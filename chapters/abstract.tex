% !TeX root = ../main.tex

\ustcsetup{
  keywords  = {高性能并行, Top-k, 国产AI处理器},
  keywords* = {High-performance parallel computing, Top-k, Domestic AI processor or China-made AI processor},
}

\begin{abstract}
  在深度学习技术蓬勃发展的浪潮中，Top-k 查询算法已深度渗透至其各个细分领域，成为众多关键任务不可或缺的核心操作环节。
  然而，深度学习模型所涉及的数据规模呈指数级增长，这使得传统的 Top-k 查询算法在应对海量数据时陷入困境，
  其效率之低难以契合当下对快速响应与精确查询的严苛需求。
  与此同时，DLP-M 系列处理器作为国产 AI 硬件架构的杰出代表，专为深度学习应用场景量身打造，具备指令集高度灵活、并行计算能力卓越以及硬件资源利
  用率出众等显著优势。遗憾的是，现有的 Top-k 查询算法，如RadixSelect，BucketSelect等算法，大多聚焦于传统 CPU 或 GPU 来进行实现和优化，
  由于体系结构和编程范式的差异，不能直接应用在 DLP-M 等国产 AI 处理器上，因此其巨大潜力还远远没有被释放。
  在此背景下，设计具备高性能并行计算能力的 Top-k 查询算子，并深度融合 DLP-M 的硬件特质以优化数据处理效能，
  已然成为当前学术研究的前沿热点与关键方向。
  
  另外，随着国家之间的科技竞争不断升级，该研究不仅能够进一步填补国产 AI 处理器在高效 Top-k 算子优化方面的空白，
  也为国产 AI 计算生态的发展提供了理论与实践支持。未来，随着国产 AI 计算体系的不断完善，
  本研究将进一步推动人工智能计算框架的本土化发展，助力自主创新与产业升级。
  鉴于此，本文基于RadixSelect算法，并紧密依托 DLP-M 的硬件特性，设计并实现了更高性能的 Top-k 算子，并在深度学习任务
  的不同场景下进行优化，充分验证了其卓越的性能优势。具体而言，本文的主要学术贡献体现如下：
  
  其一，针对大/小 k 两种差异化场景，基于 RadixSelect 算法进行深度拓展，结合 DLP-M 独特的体系结构和指令集设计并实现了 Top-k 算子，
  有效拓宽了国产 AI 处理器上 Top-k 查询方法的边界与可能性。实证研究表明，在数据量达到较大规模时，该算法相较于原有的实现方案展现出更为优异的性能表现，在效率提升方面成效斐然。
  
  其二，深度扎根于国产 AI 处理器体系架构，对 RadixSelect 并行算法展开全方位优化。通过充分发掘 DLP-M 处理器的硬件加速潜能与向量化指令优势，
  实现高效并行加速。同时，从桶操作的并行性、内存访问效率以及流水线指令等多个维度进行精细优化，
  使得算法吞吐量获得大幅跃升。在处理较大规模数据时，其性能表现已趋近于 NVIDIA A100 GPU，达到行业领先水平，为国产处理器在该领域的应用树立了新的标杆。
  
  其三，着眼于深度学习模型的实际应用需求，开展功能性验证研究。
  为切实检验算法在真实应用场景中的效果，本文精心设计了精度测试实验和性能测试实验。
  并巧妙结合 Pytorch 深度学习框架，将优化后的 Top-k 算子融入神经网络，进行训练和推理的可用性验证。
  实验数据显示，其在精度方面与基准方法维持相当水平，为深度学习任务中 Top-k 查询问题的高效解决提供了创新性的解决方案与实践路径。
  
  综上所述，本文所提出的基于 DLP-M 系列处理器的高效 Top-k 查询算法，凭借其对硬件特性的精准把握与定制化优化策略，成功实现了深度学习任务中 Top-k 操作效率的质的飞跃，为学界与业界在高效处理海量数据的 Top-k 查询问题上开辟了崭新的视野与方向，
  具有重要的理论与实践意义。 
\end{abstract}

\begin{abstract*}
  In the surging wave of the rapid development of deep learning technology, the Top-k query algorithm has deeply penetrated into various sub-fields of deep learning and has become an indispensable core operation in numerous key tasks. However, the data scale involved in deep learning models is growing exponentially, which has plunged traditional Top-k query algorithms into difficulties when dealing with massive data. Their low efficiency fails to meet the stringent requirements for rapid response and precise query in the current context. Against this backdrop, designing Top-k query algorithms with high-performance parallel capabilities and deeply integrating the hardware characteristics of deep learning processors to optimize data processing efficiency has already become the cutting-edge hotspot and key direction in current academic research.

  The DLP-M series of processors, as an outstanding representative of domestic AI hardware architectures, are tailor-made for deep learning application scenarios and possess remarkable advantages such as highly flexible instruction sets, excellent parallel computing capabilities, and outstanding hardware resource utilization rates. Unfortunately, most of the existing Top-k query algorithms focus on optimization strategies in traditional CPU or GPU environments and fail to fully unleash the huge potential of domestic AI processors. In view of this, this paper closely relies on the hardware characteristics of DLP-M and ingeniously designs and successfully implements two highly efficient Top-k query algorithms. Their outstanding performance advantages and wide application values have been fully verified in practical deep learning tasks. Specifically, the main academic contributions of this paper are as follows:
  
  Firstly, for two different scenarios of large and small k values, based on the Radix-Select algorithm, this paper conducts in-depth expansion, carefully designs and stably implements innovative Top-k query methods. Moreover, according to the unique requirements of each scenario, a multi-core implementation scheme is designed, effectively broadening the boundaries and possibilities of the Top-k query methods on domestic AI processors. Empirical studies show that when the data volume reaches a relatively large scale, this algorithm demonstrates a more excellent performance compared to the original implementation schemes, achieving remarkable results in efficiency improvement.
  
  Secondly, this paper conducts a comprehensive optimization of the RadixSelect parallel algorithm based on the domestic AI processor architecture. By fully exploiting the hardware acceleration potential and the advantages of vectorized instructions of the DLP-M processor, efficient parallel acceleration is achieved. Meanwhile, fine optimizations are carried out from multiple dimensions such as the parallelism of bucket operations, memory access efficiency, and pipeline instructions, resulting in a significant increase in the algorithm's throughput. When dealing with relatively large-scale data, its performance is approaching that of the NVIDIA A100 GPU, reaching the industry-leading level and setting a new benchmark for the application of domestic processors in this field.
  
  Thirdly, focusing on the practical application requirements of deep learning models, this paper conducts functional verification research. To effectively test the effectiveness of the algorithm in real application scenarios, this paper skillfully combines the PyTorch deep learning framework, deeply integrates the optimized Top-k query algorithm into the neural network, and conducts systematic training. Experimental data show that in large-scale datasets containing millions of candidate regions, this algorithm not only significantly accelerates the screening speed of candidate regions but also maintains a comparable detection accuracy with the benchmark method, achieving a perfect balance between efficiency and accuracy. It provides an innovative solution and practical path for efficiently solving the Top-k query problem in deep learning tasks.
  
  In conclusion, the highly efficient Top-k query algorithm based on the DLP-M series of processors proposed in this paper, relying on its precise grasp of hardware characteristics and customized optimization strategies, has successfully achieved a qualitative leap in the efficiency of the Top-k operation in deep learning tasks. It has opened up new horizons and directions for the academic and industrial communities in efficiently handling the Top-k query problem for massive data, and has important theoretical and practical significance. 
\end{abstract*}
