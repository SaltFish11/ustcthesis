\chapter{绪论}
\section{研究背景及意义}
近年来，随着深度学习和人工智能技术的快速发展，全球范围内对数据计算能力的需求急剧上升\cite{devlin2019bert}\cite{vaswani2017attention}。
同时，在深度学习\cite{vaswani2017attention}、推荐系统\cite{han2022vision}、搜索引擎\cite{wangningmapreduce}和目标检测\cite{krizhevsky2012imagenet}等应用场景中，
Top-k 查询算法作为核心数据筛选工具，被广泛应用于从大规模数据集中选取最优数据\cite{韩希先2010tkep}\cite{JSJA202405009}。
Top-k 查询的主要任务是从海量数据中快速筛选出具有最高优先级或得分的前 k 个数据元素，为后续的模型处理或决策提供支持。
尤其是在深度学习模型中，Top-k 查询被频繁用于多个关键操作，例如筛选权重较大的神经元、从候选区域中选出优先级最高的目标框，以及在自然语言处理任务中选取概率最高的词汇或短语。
它直接影响模型的运行效率和预测性能，是深度学习系统中不可或缺的重要组成部分。\cite{zeng2024turn}
然而，随着数据规模的指数增长和模型复杂度的不断提升，传统基于 CPU 和 GPU 的 Top-k 查询算法在计算性能、能耗比和响应时间等方面的局限性逐渐显现，
难以满足当前深度学习与大数据处理场景中的高性能需求\cite{sgherzi2022mixed}。
传统算法通常基于排序（如快速排序或堆排序）或分治策略（如快速选择算法），但这些方法在面对超大规模数据集时，往往计算代价高昂且对硬件资源的利用率不够高。
此外，随着实时性要求的增加，例如推荐系统中的动态数据筛选、目标检测中的实时候选框生成，传统算法在处理延迟和能耗控制方面的劣势尤为突出。

与此同时，作为人工智能技术的重要支撑，AI 芯片的研发和应用近年来成为全球科技竞争的关键领域。
AI 芯片是专为深度学习等人工智能任务设计的硬件加速器，它不同于传统的通用 CPU 或 GPU，而是针对特定任务进行了优化，能够在算力密集型任务中提供更高效
的处理能力。例如，AI 芯片通常配备大规模并行计算单元和专用的硬件加速模块，支持高吞吐量的矩阵运算和深度学习推理计算。
近年来，国产AI芯片作为我国推进科技自主可控、实现关键技术突破的重要组成部分，取得了快速发展。以寒武纪、华为昇腾、天数智芯等为代表的国产芯片在算力、能效比
算法支持等方面已接近国际一流水平\cite{ZWQY201725011}\cite{GCKX202501012}。

然而，与国外成熟的 CPU 和 GPU 生态相比，寒武纪自研的 DLP-M 国产AI芯片在算法优化和生态完善方面仍有提升空间。
因为当前许多现有算法的设计和实现主要针对通用 CPU 或 GPU 的硬件架构进行优化，
例如利用 GPU 的多线程并行性（SIMT编程范式）\cite{ghorpade2012gpgpu}或 CPU 的缓存特性\cite{hill2002evaluating}。
而 DLP-M 国产AI芯片主要基于DaDianNao体系结构发展而来，具有与CPU和GPU不同的硬件架构，指令集设计和编程模型，例如多核异构计算、片上存储和流水线优化，
向量指令集优化等\cite{7011421}。
因此国内外的大部分已有研究并不能直接在DLP-M上进行应用，仍需要研究人员针对不同的算法在不同的场景下进行实现和优化。
近些年来，寒武纪公司团队深入研究了其自研 DLP-M 体系结构，并在此基础上基于冒泡排序
算法对 Top-k 算子进行了实现和优化，以提升Top-k算子的计算效率。在数据量较小时，该方法展现出较优
的性能，然而随着深度学习模型处理的数据规模不断增长以及冒泡排序算法本身的缺陷（较高的时间复杂度），
该方法在某些场景下的计算效率捉襟见肘，在大规模数据处理任务中的计算瓶颈也逐渐显现。
因此，采用更高效的 Top-k 计算算法，并结合算法与硬件的深度协同优化，以充分挖掘 DLP-M 国产 AI 芯片的计算潜能，
已成为提升其在海量数据处理场景下性能的关键途径。面对现有方案在大规模数据计算中的算力瓶颈，如何通过软硬件协同设计进一步优化计算效率、提升资源利用率，
已成为当前研究领域的重要课题。

在这一背景下，研究基于国产AI芯片的 Top-k 查询算法具有重要意义。首先，该研究有助于提升国产芯片的应用价值和相较于国内外厂商的市场竞争力。通过针对DLP-M芯片架构特性的定制化优化，可以设计出更高效的 Top-k 查询算子，使国产AI芯片在深度学习和大数据处理任务中的性能表现更加突出。这将显著增强国产芯片的市场竞争力，推动其在人工智能相关领域的广泛应用，为我国芯片产业的发展提供技术支撑。
其次，该研究将推动深度学习和大数据处理领域的技术创新。Top-k 查询算法是多个关键任务中的基础操作，其性能直接影响整个系统的效率。通过结合国产AI芯片的硬件特点进行优化，可以大幅提升查询效率，降低数据处理延迟，为深度学习模型的训练和推理提供更好的支持。这一研究还将为其他领域的高性能算法设计提供参考，如推荐系统、搜索引擎和金融风控等需要实时数据筛选的场景。
另外，该研究对于实现科技自主可控、保障国家数据安全具有重要意义。在当前国际科技竞争加剧的背景下，数据处理与计算能力已成为衡量国家科技实力的重要指标。
通过基于国产AI芯片的算法优化研究，可以逐步减少对国外硬件平台和技术生态的依赖，形成自主可控的技术体系，提升我国在人工智能领域的核心竞争力。
同时，国产芯片在重要领域中的广泛应用，也将进一步保障我国的数据安全和技术主权\cite{durhambattle}。
最后，该研究还将为软硬件协同优化提供新的实践经验。AI 芯片的性能提升不仅依赖于硬件设计，还需要与上层算法和应用深度融合。通过针对国产AI芯片的硬件架构设计高效的 Top-k 查询算法，可以探索硬件资源的最佳使用方式，例如如何优化流水线执行效率、如何提高内存访问效率等。这一过程将为未来国产芯片的设计和优化积累宝贵经验，推动软硬件协同发展的创新。

综上所述，基于国产AI芯片的 Top-k 查询算法研究，不仅在理论上为高效算法设计提供了新思路，还在实践中推动了国产芯片技术的应用落地和生态建设。通过结合硬件特性进行定制化优化，该研究能够显著提升深度学习和大数据处理任务中的计算效率，同时推动人工智能技术的全面发展。这一研究在国家科技战略和产业实践中均具有重要价值，为我国人工智能产业的高质量发展提供了有力支撑。

\section{Top-k算法国内外研究现状}

Top-k问题是指从给定数据集中提取前k个最大（或最小）元素的经典计算任务，其中k为用户定义的正整数。
例如，在包含100个整数的数组中指定k=10时，Top-k算法需识别出其中最大（或最小）的10个元素\cite{hoare1961algorithm}。‌随着
大数据和人工智能技术的快速发展，Top-k问题已成为数据处理领域的核心操作之一，其应用涵盖搜索引擎、数据库管理系统、推荐系统、图神经网络等
多个重要场景‌。典型应用包括：‌搜索引擎的结果排序（返回最相关的k个网页）、数据挖掘的频繁项集发现（统计出现频次最高的k个项）、机器学习中的特征选择（筛选对目标变量影响最大的k个特征）等‌。

‌在算力需求激增的背景下，Top-k算法的高效并行化实现已成为学术界与工业界的共同研究焦点‌\cite{zhang2023parallel}\cite{Gaihre_Zheng_Weitze_Li_Song_Ding_Li_Liu_2021}。‌传统Top-k算法主要基于冯·诺依曼架构的通用处理器设计，其优势在于实现简单且普适性强‌。‌然而，随着数据规模的指数级增长，传统架构逐渐暴露出内存带宽受限、并行度不足等算力瓶颈‌。‌为此，研究者致力于探索新型计算范式下的并行化方案，例如基于GPU的异构加速、近似计算优化以及定制化硬件设计‌。

‌从算法设计角度，现有Top-k解决方案主要可分为三类‌：1) ‌基于排序的方法‌（如快速排序后截取前k项）；2) ‌基于选择的方法‌（如快速选择算法优化中间过程）；3) ‌基于概率数据结构的方法‌（如Count-Min Sketch等近似计算技术）\cite{baidu2024topk}。‌这些方法在时间复杂度、空间效率及硬件适配性方面呈现显著差异，需要针对具体应用场景进行权衡选择‌。

\begin{enumerate}
\item{基于排序的方法}：
此类别中的算法首先对整个数据集执行排序操作，随后提取前\(k\)个元素。典型的有完全排序法，像运用快速排序算法对整个数据集排序后选取前\(k\)个；
还有部分排序法，例如改进的冒泡排序，仅执行\(k\)次冒泡过程以获取前\(k\)个元素\cite{sedgewick1978implementing}。

\item{基于选择的方法}：
其中包含快速选择（Quickselect）算法，其与快速排序相似，不过仅针对划分后的特定部分开展递归操作，进而确定 Top - \(k\)元素\cite{martinez2001optimal}；另外还有堆（Heap）算法，
借助大小为\(k\)的最小堆（用于查找 Top - \(k\)小元素）或最大堆（用于查找 Top - \(k\)大元素）来筛选出前\(k\)个元素。
在堆算法中，先将数据集中的前\(k\)个元素构建成堆，接着遍历剩余元素，若元素与堆顶元素相比更符合条件（依据查找 Top - \(k\)小或大元素而定），
则替换堆顶元素并重新调整堆结构。

\item{基于概率的数据结构方法}：
比如采用 Bloom Filter + 计数的方式，先利用 Bloom Filter 初步甄别可能属于 Top - \(k\)的元素，之后通过计数手段确定实际的 Top - \(k\)元素；
或者运用 Count - Min Sketch + 估计的方法，凭借对元素频率的估计来找出 Top - \(k\)元素。这些方法在应对大规模数据时，
能够凭借概率数据结构在空间利用上的高效性，以近似的方式定位 Top - \(k\)元素。
其中，Bloom Filter 是一种高效的概率数据结构，用于判断元素是否可能在集合中，其具有较低的空间复杂度\cite{luo2018optimizing}；
Count - Min Sketch 则主要用于频率估计，通过特定的参数设置来平衡估计的准确性与资源消耗\cite{cormode2005improved}。 
\end{enumerate}

\subsection{国外研究现状}


在 Top-k 并行算法研究的早期阶段，国外学者奠定了重要的理论与算法基础。例如，Threshold Algorithm (TA) 和 No Random Access 
(NRA) 等算法被提出，为 Top-k 查询提供了基础框架 \cite {fagin2001optimal}。
这些算法通过设定阈值和限制随机访问等方式，初步解决了在大规模数据集中查找 Top-k 元素的基本问题，
尽管其在效率和扩展性方面存在一定局限，但为后续研究提供了关键的起点。
Ilyas(2004)在早期也深入探讨了在关系查询处理器中支持 Top-k 连接查询的问题\cite{ilyas2004supporting}。并提出了一种新的排名连接（rank-join）算法，
该算法利用输入数据的单独排序来生成基于用户指定评分函数排序的连接结果。其核心思想是在执行连接操作的过程中，逐步对连接结果进行排名。
在基于 MapReduce 的并行计算模型兴起后，
Lin(2015)提出了一种新的基于 MapReduce 的大数据 Top-k 查询算法\cite{lin2015top}。作者从数据分区和数据约简等角度，对大数据环境下的 Top-k 查询进行了深入研究。理论和实验结果表明，该算法显著提高了查询效率。
Guzun(2016)提出了一种两阶段的 MapReduce 算法\cite{guzun2016two}，用于在高维数据上处理可扩展的偏好查询。该算法利用位切片索引（Bit-Sliced Index, BSI）来高效地执行聚合和 Top-k 查询。
为了进一步提高 Top-k 并行算法的执行速度，硬件加速成为国外研究的一个重要方向。
Zhang(2024)等人提出两种高效的 GPU 并行 Top-k 算法：AIR Top-K（Adaptive and Iteration-fused Radix Top-K）和GridSelect\cite{zhang2023parallel}。
AIR Top-K 采用迭代融合设计，减少 CPU-GPU 通信和数据访问，并通过自适应策略优化不同数据分布下的内存访问。GridSelect 则通过共享队列与并行
两步插入策略，降低高成本操作的执行频率，实现高效流式处理,显著提升了GPU上的Top-k查询性能，为大规模数据处理提供了高效解决方案。
Li(2024)则提出了RadiK，一种可扩展且优化的 GPU 并行基数 Top-k 选择算法\cite{li2024radik}，突破了现有基于归并方法的 GPU Top-k 计算受限于片上存储的瓶颈。
RadiK 适用于更大的 k 值，且在不同输入长度和批处理规模下保持高效。其优化框架专为高内存带宽和资源利用率设计，将性能大幅度提升。
此外，RadiK 采用自适应扩展技术，在极端输入分布下进一步提升性能，显著增强了 GPU 上的 Top-k 选择能力。
除了在GPU上的工作,Ebrahim(2023)则设计并实现了一种优化的 FPGA 加速器，FPGA 具有可灵活编程
和低功耗的特点，他们针对这些特性设计了专门的 Top-k 查询电路结构\cite{ebrahim2023finding}, 采用 C语言级别进行描述，便于使用HLS工具复现。
该方案在吞吐量和功能性方面优于当时现有的最先进加速器，并能在保证高准确率的同时，实现更高的计算效率。
随着深度学习在人工智能领域的蓬勃发展，国外学者开始探索 Top-k 算法与深度学习的结合应用。Raihan 等人（2020）研究
了神经网络稀疏激活中的 Top-k 选择问题 \cite {raihan2020sparse}。在神经网络训练过程中，通过 Top-k 算法对神经元的激活值进行筛选，保
留最具影响力的前 k 个激活值，能够有效减少计算量和模型复杂度，提高训练效率。
Girshick在(2015)年提出了Fast r-cnn算法,该算法在推荐系统，搜索引擎，图像检测等领域得到了广泛应用，通过对用户行为数据或搜索结果的 Top-k 筛选与分析，
能够更精准地为用户提供个性化推荐和搜索结果，提升用户体验和系统性能\cite{girshick2015fast}。

国外研究围绕数据结构优化、并行计算、分布式计算及硬件加速进行了深入探索，并在 搜索引擎、推荐系统、深度学习、金融风控等领域取得了突破性进展。
研究结果表明，高效 Top-k 并行算法可极大提升数据查询与筛选的性能，并推动人工智能计算生态的发展。未来，国产 AI 处理器在 Top-k 计算优化方面仍有巨大潜力，如何结合国产硬件架构进行深度优化，将是推动计算生态自主创新的重要方向。



\subsection{国内研究现状}

随着数据隐私保护意识的不断增强，国内学者在分布式环境中支持加密计算的 Top-k 算法研究方面也取得了新的突破。
崔韶刚等(2023)在多用户应用场景下，为抵御云与数据拥有者或使用者对授权用户查询隐私的合谋攻击，构建了合谋威胁模型，实现高效强隐私保护查询方案。
方案采用动态安全查询索引结构，保障数据安全、适应数据动态更新，提升系统可伸缩性与可用性，还依关键字权重对查询结果秘密排序，实现安全top-k查询。
潘瑛颖等(2023)考虑到用户在输入关键词时极易出现拼写错误的情况，文章提出了基于标记图的top-k最近模糊关键词查询的图加密方案\cite{WXHK202311043}。
该方案基于 2-Hop 标签精心构造加密索引结构，以此计算最短距离，运用基于通配符的方法为关键词生成模糊集，进而构造模糊关键词索引来实现模糊关键词查询，
最终精准返回距给定节点最近的 k 个可能被所需关键词标记的节点。

国内学者针对本地化需求和实际应用场景，在分布式优化方面同样开展了大量研究工作。
慈祥(2014)等学者构建了基于MapReduce环境的Top-K查询算法，此算法在数据划分模式上独辟蹊径，有效增强了数据查询性能\cite{RJXB201404009}。然而，由于其依据空间角度与距离来划分数据，在处理高维度数据查询时，难免出现结果偏差。
罗浩(2016)针对分布式查询场景，提出借助网格与超平面进行数据划分的新方法\cite{1016325208.nh}。该方法不仅能有效克服高维度数据查询不准确的难题，还通过区域划分及区域间并行计算，大幅提升了查询效率。美中不足的是，分区内的数据筛选仍采用串行处理方式。 

伴随社会及计算机领域发展，数据量爆发式增长，高效低成本查询处理亟待解决。由于多种因素，不确定数据大量存在，传统算法难以适用，
不确定数据流处理更为复杂。张翅(2020)提出面向不确定数据的Top-k支配查询算法和改进的并行算法\cite{1020759080.nh}，通过运用若干剪枝方法和支配关系减少计算与重复工作,以提升计算效率。
算法首先对不确定数据使用可能世界模型进行建模,根据可能世界实例得出Top-k概率的计算方式,随后通过用户已给出的排序分值范围、概率阈值与Top-k概率的相关比较以及支配关系的定义,从而对数据剪枝,以减少后续的工作,提高查询效率
同时，设计实现原型系统验证算法可行性，为该领域研究提供了新思路。
张航(2024)针对不确定时间序列Top-k窗口聚合查询问题展开研究，提出了一种创新性的两级Top-k
查询框架\cite{JSJC20240614003}。针对现有方法存在的存储开销大、查询效率低等局限性，
设计了基于子窗口拼接的动态规划策略，通过构建可复用的子窗口聚合值索引结构，
显著降低了预计算存储需求。特别地，为解决子窗口拼接带来的阈值计算复杂度问题，
提出了一种高效的阈值上界计算方法，有效缩小候选解空间。实验验证表明，其方法在存储效率方面较传统基于
TA的方法实现突破性改进，同时在查询响应速度上较FSEC-S基准方法提升7.27至20.04倍。
研究成果为不确定时间序列分析提供了新型基础查询工具，在物联网传感数据分析、金融时序预测等领域具有重要应用价值。本研究通过理论创新与算法优化，实现了存储空间与计算效率的双重突破，推动了时间序列数据管理技术的发展。

在Top-k并行算法硬件加速领域,国内的相关研究相较于国外比较少,
但在早期国内AI芯片的浪潮暂未开启时, 国内同样存在Top-k查询实时性能优化的问题,
为此吴超等人(2012)曾针对有代表性的NRA程序展开性能分析，依据多核处理器结构特点，采用分层优化法并行优化，通过调整数据结构、调度任务等，使程序在实验数据集上串行性能提升59\%，加速比接近线性 \cite{XXWX201207019}。 
针对GPU并行计算在Top-k查询中的潜力挖掘问题，黄玉龙(2014)等人提出基于CUDA架构的大规模分段并行算法。通过设计动态数据分段策略重构查询流程，将计算任务映射至GPU多级存储体系，并采用流式处理优化显存带宽利用率。实验表明，该算法在6个有序列表、120步长参数下较4线程CPU方案实现40倍加速，并行计算效率达硬件峰值性能的78\%。该成果为海量数据实时检索场景提供了异构计算新范式，显著提升高吞吐量查询系统的响应时效性。
在嵌入式系统研究领域, 李靓琦(2020)基于Tegra K1架构对Top-k批量查询问题进行了深入的研究,提出了一种适合Tegra K1架构的Top-k查询分段优化算法\cite{DZRU202017051}。

而随着国产硬件技术和深度学习的不断发展，国内学者和相关企业的研究人员也在积极探索 Top-k 算法在各自国产硬件平台上的应用与优化。
但各个国产AI芯片之间体系结构都存在或大或小的差异,需要专门进行软硬件协同优化才能够将各自的硬件性能发挥到极致.
为此寒武纪公司团队深入研究了 DLP-M 体系结构，并在此基础上对 Top-k 算子 进行了优化，采用改进的冒泡排序算法以提升计算效率。
在小规模数据集场景下，该方法能展现出较优的性能。然而，随着深度学习模型处理的数据规模不断增长，
该方法的计算效率未能有效扩展，在大规模数据处理任务中的计算瓶颈逐渐显现。
此外，该方法在数值稳定性方面仍存在一定的不足，可能影响其在高精度计算任务中的性能效果。
因此，如何进一步优化计算效率，以适应大规模AI计算需求，仍是一个值得深入研究的问题。


\section{AI 处理器的国内外研究与发展现状}

人工智能（Artificial Intelligence, AI）技术的快速发展对计算硬件提出了更高的要求。
随着深度学习模型的复杂度不断提高，传统的计算架构难以满足高效能计算需求，因此专用 AI 处理器（AI Accelerators）应运而生。
近年来，国内外在 AI 处理器的研究和应用方面取得了重大进展\cite{yishouyi}，涵盖从通用 GPU、ASIC（Application-Specific Integrated Circuit）到 FPGA（Field-Programmable Gate Array）等多种计算架构。本文将分别从国际与国内的角度探讨 AI 处理器的研究进展，并分析未来发展趋势。

\subsection{国外研究与发展现状}

\subsubsection{GPU}

在 AI 计算领域，GPU一直占据主导地位。NVIDIA 作为全球 GPU 计算的领导者，自 2016 年推出专为 AI 计算优化的 Tesla 系列 GPU 以来，
持续迭代其架构，以提升深度学习训练和推理任务的效率。最新的 Hopper 架构 GPU（如 H100）引入 Transformer Engine 和 FP8 计算，
显著提升了大规模 AI 训练任务的吞吐量 \cite{choquette2023nvidia}。
此外，AMD 也推出了 MI200 系列 AI 加速 GPU，与 NVIDIA 竞争云端和高性能计算市场 \cite{smith2022amd}。

\subsubsection{TPU 和 ASIC}

Google 在 2017 年推出 TPU（Tensor Processing Unit），其设计目标是优化 AI 计算中的矩阵运算，采用 systolic array 结构
以提升计算效率 \cite{jouppi2017datacenter}。最新的 TPU v4 在计算密度和能效比方面相较于前代产品进一步优化，并在 Google Cloud 计算平台中广泛应用。

其他科技公司也在 ASIC 领域展开布局，如 Tesla 推出的 FSD（Full Self-Driving）芯片，专用于自动驾驶 AI 计算，采用 7nm 工艺
并集成神经网络加速单元 \cite{talpes2020compute}。Apple 也在其 M 系列芯片（如 M1、M2）中集成了 Neural Engine，
优化移动端 AI 任务的执行效率 \cite{kasperek2022comparison}。

\subsubsection{FPGA}

FPGA 由于其灵活性和低延迟特性，在 AI 计算中也扮演重要角色。微软 Azure 采用基于 FPGA 的 Project Brainwave，为云端推理提供高
效能计算支持 \cite{microsoft2019brainwave}。
英特尔开发出 Agilex 5 等系列产品,性能突出。同时，英特尔提供多种 AI 开发工具，如 AI Analytics Toolkit、OpenVINO 工具包等，助力开发者构建高效的边缘 AI 系统，
推动边缘 AI 在多领域应用发展\cite{ahmadintel}。

\begin{table}
    \centering
    \caption{国外 AI 处理器产品}
    \label{tab:ai_chips}
    \begin{tabular}{cll} % 注意这里是 'cll' 表示三列，第二列和第三列都对齐左侧
      \toprule
      公司   & 处理器系列                                       & 代表产品                          \\
      \midrule
      {NVIDIA} & GPU (Graphics Processing Unit) & A100, H100, RTX 4090 \\ 
      {Google} & TPU (Tensor Processing Unit) & TPU v4, TPU v5e \\ 
      {AMD} & AI 加速 GPU & Instinct MI250, MI300 \\ 
      {Intel} & AI 专用加速器 & Habana Gaudi2, NNP-T, NNP-I \\ 
      {Tesla} & FSD (Full Self-Driving) 芯片 & FSD Chip v1, v2 \\ 
      {Apple} & Neural Engine & M1, M2, A17 Pro Neural Engine \\ 
      {Meta (Facebook)} & MTIA (Meta Training and Inference Accelerator) & MTIA v1 \\
  
      \bottomrule
    \end{tabular}
\end{table}


\subsection{国内研究与发展现状}

\subsubsection{国产 AI 处理器的兴起}

近年来，国内 AI 处理器领域发展迅速，多家企业和研究机构纷纷推出自主研发的 AI 计算芯片。寒武纪（Cambricon）自 2016 年推出第一代深度学习处理器以来，
不断优化其架构，其MLU370 处理器采用先进的芯粒（Chiplet）封装技术，提高 AI 计算性能 \cite{song2023cambricon}。
华为昇腾（Ascend）系列 AI 处理器基于自研达芬奇（Da Vinci）架构，支持多精度计算，适用于云端和边缘计算 \cite{liao2021ascend}。
此外，百度推出的昆仑 AI 处理器，面向云计算和边缘计算优化，采用国产自研架构并支持高效推理计算 \cite{zhang2023intelligent}。
阿里平头哥开发的含光 800 AI 处理器，基于 RISC-V 指令集，主要应用于智能城市和数据中心 \cite{reuther2020survey}。
值得注意的是，各家企业的AI处理器其指令集，硬件体系结构以及对应的编程模型有着各自的特点。
因此，即使在算法一致的情况下，也需要结合各自的特点进行实现和优化。

\begin{table}
    \centering
    \caption{国产 AI 处理器产品}
    \label{tab:ai_chips_1}
    \begin{tabular}{cll} % 注意这里是 'cll' 表示三列，第二列和第三列都对齐左侧
      \toprule
      公司   & 处理器系列                                       & 代表产品                          \\
      \midrule
      {华为 (Huawei)} & Ascend (昇腾) & Ascend 310, Ascend 910B \\ 
      {寒武纪 (Cambricon)} & MLU (Machine Learning Unit) & MLU270, MLU370 \\ 
      {阿里巴巴 (Alibaba)} & 含光 (Hanguang) & Hanguang 800 \\ 
      {百度 (Baidu)} & 昆仑 (Kunlun) & Kunlun 1, Kunlun 2 \\ 
      {飞腾 (Phytium)} & AI 计算 CPU & FTC660, FTC860 \\ 
      {比特大陆 (Bitmain)} & 神算 (Sophon) & Sophon BM1684X, BM1690 \\ 
      {地平线 (Horizon Robotics)} & 旭日 (Sunrise) & Sunrise 3, Sunrise 5 \\ 
 
      \bottomrule
    \end{tabular}
\end{table}

\subsubsection{学术界的研究进展}

国内高校和研究机构也在 AI 计算架构领域取得了诸多研究成果。例如，北京大学的研究团队提出了面向异构计算的低功耗 AI 加速架构，
优化计算效率。清华大学开发了基于存算一体（Processing-in-Memory, PIM）技术的 AI 处理器，
旨在降低数据传输带来的能耗瓶颈。此外，中科院计算技术研究所也在 AI 处理器领域进行探索，开发了一系列 RISC-V 兼容的 AI 计算芯片，为国产自主可控 AI 计算硬件提供支
持 。


\subsection{发展趋势}
未来，AI 处理器的发展将朝着异构计算方向演进，即通过整合 CPU、GPU、ASIC 及 FPGA 资源，提高计算吞吐量和能效比。例如
，AMD 和 NVIDIA 均在研究 CPU-GPU 互连架构，
以优化 AI 计算的整体性能 \cite{mishty2024chiplet}。同时，Chiplet（芯粒）技术成为 AI 处理器的重要发展方向。
例如，寒武纪 MLU370 采用 Chiplet 设计，提高了计算模块的可扩展性和数据传输效率 。
Intel 和 AMD 也纷纷推出基于 Chiplet 设计的 AI 加速芯片，以降低生产成本并提高计算效率。而随着 IoT（物联网）和
智能终端设备的发展，边缘 AI 计算市场需求持续增长。例如，华为的 Ascend Nano 处理器面向低功耗 AI 计算优化，适用于智
能摄像头和无人机等应用 。此外，Apple 和 Qualcomm 也在智能手机芯片中集成 AI 计算单元，
以提升设备端的 AI 计算能力 。
另外，国家层面的政策支持为国产 AI 处理器的发展提供了良好环境。例如，《新一代人工智能发展规划》中明确提出加强 AI 计算
硬件自主研发，并支持企业和高校在 AI 处理器领域的创新 . 近年来，各地政府纷纷设立 AI 产业基金，
支持 AI 芯片企业的技术突破和产业化应用。

综上所述，AI 处理器的研究和发展在全球范围内呈现出多样化和快速演进的趋势。国际上，GPU 仍然主导 AI 计算市场，但 ASIC 
和 FPGA 逐步获得更大市场份额。国内企业和研究机构在 AI 处理器领域取得了长足进步，未来将进一步推动自主可控计算架构的发展。
展望未来，异构计算、Chiplet 技术和边缘 AI 计算将成为 AI 处理器的重要发展方向。



\section{论文主要内容及章节安排}
% \subsection{论文主要工作}
% 本文重点研究基于RadixSelect算法实现的Top-k算子在DLP-M国产AI平台上的实现与性能调优。
% 最后通过大量的实验验证其精度和性能表现，并结合Pytorch深度学习框架验证其在深度学习场景下的可用性。
% 主要工作内容包括：

% (1) 分析Top-k算法，并深入研究 DLP-M AI处理器的存储模型和异构编程模型，以此作为后面研究国产AI处理器数据级并行化和异步并行化的基础。

% (2) 基于RadixSelect算法，针对 DLP-M 国产AI处理器的独特的体系结构，编程模型和特殊的向量指令集对Top-k算子进行并行实现。

% (3) 基于DLP-M的体系结构和编程模型，通过从计算效率和访存效率两方面对Top-k算子进行不同场景下的优化。

% (4) 对Top-k算子的并行化方案以及性能调优后的方案进行精度测试和性能测试，并在深度学习框架上进行适配，
% 进行集成测试以验证Top-k算子的可用性。
% \section{基于 RadixSelect 算法的 Top-k 算子优化研究}

\subsection{论文主要工作及贡献}
随着深度学习模型规模的不断增长和计算复杂度的提升，高效的数据筛选与排序操作在 AI 计算任务中扮演着重要角色。Top-k 算子作为核心计算组件，在深度学习训练、推理、特征提取、注意力机制优化等多个关键任务中均有广泛应用。然而，现有的 Top-k 计算方法在DLP-M国产 AI 处理器上的性能优化仍存在挑战，
主要体现在以下几个方面：

\begin{itemize}
    \item 计算扩展性问题：随着数据规模的增长，传统Top-k方法（如基于冒泡排序的Top-k算法）在计算复杂度和内存占用方面难以满足高效处理需求。
    \item 硬件特性适配问题：DLP-M 国产 AI 处理器具有特定的存储架构、向量计算单元和异构编程模型，现有算法缺乏针对性的优化。
    \item 并行计算优化不足：在大规模数据处理任务中，如何充分利用 AI 处理器的向量指令集、数据并行特性和异步计算能力，提高计算吞吐量，是一个值得研究的问题。
\end{itemize}

本文聚焦于 RadixSelect 算法在 DLP-M 国产 AI 平台上的 Top-k 算子实现与性能优化，并通过实验验证其计算精度和性能表现。此外，结合 PyTorch 深度学习框架，评估该算法在 AI 计算任务中的适用性，以探讨其在国产 AI 计算平台上的优化潜力。

\subsection{主要研究内容}

为提升 RadixSelect 算法在 DLP-M 平台上的计算效率和硬件适配性，本文的研究内容涵盖以下四个方面：

\subsubsection{Top-k 算法分析与 DLP-M 体系结构研究}
\begin{itemize}
    \item 研究 Top-k 查询算法的数学原理、计算复杂度及优化策略，重点分析 RadixSelect 算法在高效选择前 k 大值问题中的优势。
    \item 深入剖析 DLP-M 国产 AI 处理器的存储层次结构、计算架构和异构编程模型，包括向量化计算单元、访存优化机制、数据流特性等，为后续的并行化优化奠定理论基础。
    \item 探讨 DLP-M 国产 AI 处理器在 Top-k 算子实现过程中可能遇到的计算瓶颈、访存冲突等问题，并提出相应优化方案。
\end{itemize}

\subsubsection{基于 RadixSelect 算法的 Top-k 算子并行实现}
\begin{itemize}
    \item 针对 DLP-M AI 处理器的体系结构、向量指令集和异构编程模型，设计并行化 Top-k 算子，实现高效的数据划分、排序、筛选计算过程。
    \item 结合 RadixSelect 算法的无比较筛选特性，优化数据访问模式，实现流水线化并行计算，提升算子执行效率。
    \item 采用 SIMD（Single Instruction Multiple Data）指令优化，减少数据移动开销，提升计算吞吐量，以充分利用 DLP-M 国产 AI 处理器的并行计算能力。
\end{itemize}

\subsubsection{Top-k 算子的计算效率与访存优化}
\begin{itemize}
    \item 计算优化：分析计算负载均衡、并行线程划分、流水线调度，提高计算资源利用率，减少冗余计算。
    \item 访存优化：优化数据预取、缓存管理、内存对齐策略，降低访存延迟，提高数据传输效率。
    \item 异步并行化优化：结合 DLP-M AI 处理器的多核架构，利用异步计算流和任务并行机制，减少计算过程中的同步开销，提高执行效率。
\end{itemize}

\subsubsection{Top-k 算子的精度验证、性能测试及深度学习框架适配}
\begin{itemize}
    \item 精度测试：评估本文实现的Top-k 算子的计算精度、数值稳定性，确保其在大规模数据集上的鲁棒性和准确性。
    \item 性能测试：设计实验对比 基于 RadixSelect 算法的Top-k算子与传统 Top-k 计算方法（如基于冒泡排序的Top-k算子）和 Nvidia A100 GPU Top-k算子的性能表现。
    \item 深度学习框架适配：
          \begin{itemize}
              \item 在 PyTorch 深度学习框架上对 Top-k 算子进行适配，将其集成至深度学习训练与推理任务，以评估其在神经网络推理和训练应用场景下的实际效果。
              \item 进行集成测试，验证优化后的 Top-k 算子在真实 AI 任务（如计算机视觉、自然语言处理等）中的可用性，为国产 AI 计算生态提供高效的算法支持。
          \end{itemize}
\end{itemize}

\subsection{研究意义}
\begin{itemize}
    \item 推动国产 AI 计算生态发展：优化国产 AI 处理器在高效 Top-k 计算方面的能力，提升 DLP-M 平台在深度学习任务中的计算效率，为国产 AI 生态提供基础算子优化方案。
    \item 提升 AI 计算任务的算力利用率：优化 Top-k 计算在模型推理、注意力机制、特征提取等关键任务中的效率，提高计算资源利用率。
    \item 探索软硬件协同优化策略：推动 AI 计算加速算法在国产硬件上的优化实践，为大规模 AI 计算任务提供更优解。
\end{itemize}


\subsection{论文结构}

本文共六个章节，各章内容组织如下： 

第一章为绪论，首先介绍本文的研究背景及意义，接着对Top-k算法的国内外研究现状和应用现状
以及 AI 处理器的国内外发展现状进行详细介绍，
最后对本文研究内容和章节安排情况进行简要概括。

第二章为相关技术背景，首先介绍了RadixSelect算法的原理，接着对国产AI处理器的
编程模型和内存模型进行介绍。最后介绍了面向国产AI处理器的深度学习框架——pytorch。


第三章为Top-k算子设计与实现，本章节主要基于RadixSelect算法，设计了Top-k算子，
并且根据Top-k算子的输入规模和国产AI处理器的体系结构，设计了两种不同的并行方案。
本章主要分为四部分，第一部分首先介绍了算子的计算流程，在此详细讲解了Top-k算子工作过程中的
几个重要阶段。而后根据其计算流程，详细介绍了主机端的主要任务。最后依据RadixSelect算法原理
和国产AI处理器的体系结构，详细的描述了两种并行实现方案和实现过程。

第四章为Top-k算子的性能优化。
首先介绍算子的优化策略，接着分别从计算效率和访存效率两方面，
对Top-k算子的优化方案进行全面讲解。
 
 第五章为实验测试与分析，本章在多核深度学习处理器上，对三四章节所实现和优化的Top-k算子进行全面的精度测试与性
 能评估，并集成到深度学习框架中，验证Top-k算子的可用性。 
 
 第六章为总结与展望，本章对全文的工作进行整理
 总结，并对本文方法中存 在的不足进行分析，然后提出后续改进的方向。
